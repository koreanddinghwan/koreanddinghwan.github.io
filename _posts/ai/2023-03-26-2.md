신경망(1) : 퍼셉트론  
두뇌 구조를 모방한 기본 신경망인 Perceptron이해하기

---

# Machine Learning

## 정의

- 데이터, 경험으로부터 모델을 구축하는 기술을 의미.
  - 파라미터를 자동 구축(확률을 통해서)
  - 모델의 구조를 자동으로 결정(BN graph를 통해 그래프의 구조, 모양)
  - 데이터에 감춰진 개념(clustering을 통해)

## Perceptron for classification

<img width="597" alt="스크린샷 2023-03-26 14 08 03" src="https://user-images.githubusercontent.com/76278794/227756520-af865176-150d-4e9a-b9bc-1ae23a90b677.png">

- Data를 이용해서 `Model Construction`을 하고,
- 만들어진 모델에 대해 `Test Data`를 통해 모델 검증을 한다.

<br>

## Examples

### 숫자 인식

- 이를 위한 구성요소는 다음과 같다.

- input : Images/pixel grids
- output : a digit 0 ~ 9
- Setup:
  1.  숫자가 라벨링된 대량의 이미지 수집(라벨:각 이미지 별로 어떤 숫자인지 명시된)
  2.  누군가가 일일히 데이터에 대해 라벨링을 해야한다.
  3.  미래의 라벨링되지 않은 데이터의 라벨을 예측
- Features: 숫자를 결정하는데에 사용되는 속성
  - Pixels: (6, 8) = ON (특정 픽셀좌표에 색이 있다 or 없다)
  - Shape patterns: 컴포넌트 숫자, x:y비율, loop개수(선의 휨)

<br>

### Spam email Filter

- input : an email
- output: spam/ham

- Setup:

  - 스팸메일인지 아닌지 라벨링된 대량의 이메일
  - 누군가가 이 모든 데이터에 라벨링을 해두어야함.
  - 새로운 미래의 이메일에 대해 라벨을 예측

- Features: 스팸메일인지 아닌지 결정하는 특성
  - 특정 단어: FREE
  - 텍스트 패턴: $dd, CAPS
  - 텍스트가 아닌 것: 발신자 정보

<br>

### Others Examples

- Classification의 정의 : 입력 x가 주어졌을때, x에 대한 label(classes) y를 예측한다.
- Classification은 상업에서 중요한 기술이다.

<br>

- Example:(input, classes)
  - OCR:(이미지, 단어)
  - Medical diagnosis:(증상, 병명)
  - Automatic essay grading:(문서, 성적)
  - Fraud detection:(계정 활동, 사기 혹은 사기가 아님)
  - Customer service email routing
  - Computer aided diagnosis:
    - 의료분야 Input 이미지, 영상 등으로 진단
  - 이미지 인식
    - 공장 자동화, 로봇 네비게이션, 얼굴인식, 모션인식, 자동 타겟 인식
  - 음성 인식
    - 화자 식별, 음성 인지

<br>

- 생체인식 분야
  - `불변의 생체 특징`을 이용한 사람 식별
  - 정적 패턴: 지문, 홍채, 얼굴, 장문, DNA 등
  - 동적패턴 : 서명, 성문, 타이핑 패턴
  - 이런 생체인식 분야는 출입통제, 전자상거래 인증 등에 사용된다.

<br>

- 제스쳐 인식 분야
  - 펜 컴퓨터에서 텍스트 편집
  - 제스쳐 입력을 통한 제어
  - 손의 모션으로 TV 컨트롤
  - 수화 해석기

<br>

## 중요 개념

### `Data`

- 스팸메일인지 아닌지에 대한 Labeling된 객체
- 이 데이터는 3가지로 나눠진다.
  1.  `Training set` : 모델 만들 때 사용
  2.  `Held out set` : 여력이 있을때, 학습할때 만드는 모델에 추가적인 파라미터(hyperparameter)를 결정할 때 사용. Training set데이터 사용하기도함.
  3.  `Test set` : `모델의 유용성 판단, 절대로 training set의 데이터 사용하면 안됨`

<br>

### `Features`

<img width="602" alt="스크린샷 2023-03-26 20 56 07" src="https://user-images.githubusercontent.com/76278794/227773960-963bc0ea-7003-4de6-9faf-5546ede0f034.png">
- 입력된 데이터 각 x를 잘 특성 지을 수 있는 `속성-값` 쌍
- 일반적으로 feature vector라고 표현한다.

<br>

### `Experimentation cycle`

1. training set에 대해 learn parameters(model probabilites)
2. `hyperparameter`조정 : 추가적인 파라미터 결정에 사용
3. test set으로 `Accuracy` 계산
4. 중요!: `test set의 데이터로 학습하면 절대로 안된다`

<br>

### `Evaluation`

- Accuracy: 정확하게 예측된 인스턴스의 비율

<br>

### `Overfitting and Generalization`

- overfitting: 학습데이터에 모델이 너무 잘 맞는 경우.
- `overfitting된 경우, Generalization`이 떨어져 실 적용이 어렵다.

<br>

- `상대빈도(relative frequency)` 파라미터를 사용하는 것은 과적합(overfitting)문제를 일으킨다.
  - 학습 사이클에서 (15,15) 픽셀이 3인 것이 없다고해도, test set에 없는것은 아니다.
  - "minute"라는 단어가 나오면 100% 스팸메일이다.
  - "seriously"라는 단어가 나오면 100% 스팸메일이 아니다.
  - `상대빈도를 기반으로 모델을 만들면 training set에는 맞을 수도 있으나 현실에서는 적용되기 힘들다.`

<br>

- 극단적으로, 이메일 전체를 하나의 특징으로 사용했다면
  - training data는 완벽하겠지만
  - generalize하지는 않을 것이다.
  - `bag-of-words assumption`을 통해 문서 내의 단어의 빈도를 통해 문서를 특징짓는 것은 약간의 generalization을 주지만, 충분하지는 않다.

<br>

- 따라서 generalize를 위해서는 `regularize`가 필요하다.
  - 일부로 틀린 데이터를 제공

<br>
<br>

## Representative Models

<img width="549" alt="스크린샷 2023-03-26 21 12 51" src="https://user-images.githubusercontent.com/76278794/227774829-79152304-e8f9-45ec-9efd-134fe4624896.png">

### 학습 분류

<br>

- 지도 학습: labeling o

  - Classification:for 분류, 카테고리값

    - Naive Bayes Classifier, DT, SVM, RF, KNN 등

  - Regression:for 특정 값 예측, 실제 value

    - Linear Regression, Neural Network Regression, Support Vector Regression, DT regression, Lasso regression, Ridge regression 등

- 비지도 학습: labeling x

  - Clustering

    - K-Means Clustering, DBSCAN 등

- 강화학습: 주어진 데이터에 대한 output 값 판단

  - Decision Making

    - Q, R, TD Learning

<br>

### 알고리즘

| Algorithms                   | Description                                                                                                            |
| ---------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| Naive-Bayes(NB)              | 유사도화 선호도를 기반으로 사후확률 예측, 각 feature들이 독립적이라는 가정을 하는 Bayes 이론을 기반으로 한다.          |
| Bayesian Networks(BN)        | 확률 값이 다른 확률의 input이 되는 인과관계 체이닝을 표현하는 모델                                                     |
| Multilayer Perceptron(MLP)   | 두뇌 구조를 모방한 다층 퍼셉트론                                                                                       |
| Support Vector Machines(SVM) | 한 클래스의 데이터 점을 다른 클래스의 데이터 점과 가능한 한 가장 잘 구분해내는 초평면을 찾는 것입니다.                 |
| Decision Trees(DT)           | 트리구조를 기반으로한 모델로, 샘플 데이터를 특징에 따라 분류                                                           |
| Random Forests(RF)           | DT의 성능을 개선, DT를 여러개 생성하는데, 랜덤하게 DT를 만들고 분류 결과가 DT보다 좋다.                                |
| K-Nearest Neighbors(KNN)     | 모델을 따로 만들지 않고, 미래 데이터를 학습 데이터 중 가장 가까운 K개의 학습데이터 label을 보고 미래 데이터를 labeling |

<br><br>

## Overview

  <img width="594" alt="스크린샷 2023-03-26 21 17 23" src="https://user-images.githubusercontent.com/76278794/227775076-3191745a-f141-47fb-96d9-00f6f611de7e.png">
- 16 \* 16 pixel의 그림이 있다.
- 각 좌표에는 ink가 있냐 없냐로 값이 있으며, 256길이의 벡터에 0 또는 1 값이 들어간다.
- 256개의 값을 기반으로 0부터 9까지 y1, y2 ... y10에 매핑하는 함수 `f`가 필요한데, 이 함수 f가 `deep learning에서 neural network`로 표현된다.

<br><br>

# Neural network

## 신경망의 정의

### 뉴런

<img width="366" alt="스크린샷 2023-03-26 21 20 54" src="https://user-images.githubusercontent.com/76278794/227775263-0da88e9d-87b7-4c2d-bd6d-c0883c63e23d.png">

- Dendrite: 이웃 뉴련에서 전기 신호를 받는다.
- Synapse : 다른 뉴런과 dendrite의 연결 부위에 있고, 전기신호 세기 조정
- Axon : soma의 전위가 일정 이상이 되면, 이웃 뉴런에 전기 신호를 보낸다.
- cell body : dendrite로부터 받은 여러 전기신호를 합친다.

<br>

### 역할

<img width="468" alt="스크린샷 2023-03-26 21 25 20" src="https://user-images.githubusercontent.com/76278794/227775467-00f16dd9-43c6-4b0f-9039-8ffaf77d548d.png">

- node 1개
- 다른 neuron들로부터 input을 받는데, 이들의 가중치 weight은 synapse에 의해 결정된다.
- 이 값들의 합이 soma에 누적되다가, 일정 이상이 되면 Axon이 이웃 뉴런에 전기 신호를 보낸다.
- 결국, 이걸 2차원 평면으로 나타내면 아래와 같은 그림이 된다.
  <img width="284" alt="스크린샷 2023-03-26 21 28 28" src="https://user-images.githubusercontent.com/76278794/227775647-1b17f3c5-f244-4d59-b597-41ca06f83183.png">
- 함수의 값이 0보다 크면 직선 위에 있으므로, y = 1, 아니면 0으로 출력한다.

<br>

### 신경망에서 뉴런

- 결국, 신경망에서 여러 뉴런을 사용하고, 여러 층을 사용하게된다.
  <img width="489" alt="스크린샷 2023-03-26 21 30 54" src="https://user-images.githubusercontent.com/76278794/227775791-be94bb7a-f99a-4e6d-b00b-ddd7f01fe28f.png">

<br>

- 기하학적으로 여러개의 신경망이 의미하는 바를 표현하면 아래와 같다.
  <img width="488" alt="스크린샷 2023-03-26 21 32 36" src="https://user-images.githubusercontent.com/76278794/227775871-271109de-fa72-44f7-920d-a3c9c795faa8.png">

<br><br>

## 심층 신경망

- 사진으로 고양이가 있는지 없는지 확인
  <img width="497" alt="스크린샷 2023-03-26 21 43 59" src="https://user-images.githubusercontent.com/76278794/227776507-ac54d9cc-08f4-44b3-9143-1949d7e611ec.png">

<br><br>

## 학습 알고리즘

### 학습의 정의

<img width="372" alt="스크린샷 2023-03-26 21 45 57" src="https://user-images.githubusercontent.com/76278794/227776614-b7edc1ec-2d61-49ca-86e9-a485ff6ec9b2.png">

- 입력에 대한 올바른 출력값을 위한 가중치를 잘 결정하는 것을 의미.
- 각 입력에 대한 출력값을 확인하고, 출력이 올바르게 되도록 가중치를 계속 바꾸는 것이 학습

<br>

### 가중치를 어떻게 바꿀 것인가

<img width="363" alt="스크린샷 2023-03-26 21 46 31" src="https://user-images.githubusercontent.com/76278794/227776637-b12702d6-126c-4b6d-af53-09f9d6ad9b28.png">

- 어떻게 바꾸는 것이 가장 좋은가가 핵심
- Loss를 줄이는 방향으로 한다.
- X가 가중치, Y가 에러값이라고 생각했을때, 위의 에러 함수의 가장 낮은 값이 위치하는 것이 Loss가 가장 작은 것.

- 하지만, 이 함수에서 가장 낮은 에러함수의 값을 찾는 것이 어렵기 때문에 `gradient`방법을 사용한다.
- `현재 point를 미분해서 gradient를 구하고, 학습률(learning rate)만큼 구한다.`

<br><br>

## linear classifiers

<img width="407" alt="스크린샷 2023-03-26 21 51 49" src="https://user-images.githubusercontent.com/76278794/227776935-7b8910ad-83bc-47d8-832b-8d263a5af9eb.png">
- Input x에 대한 feature value와
- 각 feature의 가중치 weight을 모두 더한다.
- 이렇게 가중합하여 activation을 구한다.

<br>

- 가중합한 activation이 양수인지 아닌지에 따라 output이 결정된다.
- 로지스틱 회귀와 완전히 동일한 역할을 한다.

<br><br>

## weights

<img src="https://user-images.githubusercontent.com/76278794/227793021-37146788-6f84-4741-afae-6fd5ea650efc.png">

- binary case: 스팸메일인지 아닌지 확인하는 케이스라고 가정한다.
- w는 기존에 있던 데이터이다.
- 새로운 데이터 x1을 함수 f에 넣었을때, 그 결과를 w와 내적한다.
- 내적해 두 벡터(w와 f(x1)) 사이 각도를 구해 90도 미만이면 양의 값으로 같은 클래스로, 90도가 넘어가면 다른 클래스로 분류한다.

<br><br>

### Binary Decision Rule

<img src="https://user-images.githubusercontent.com/76278794/227793181-9c51ff89-4f3e-4a5f-9351-691d358e57f9.png">

- 데이터를 각 point이다.
- 초평면, hyperplane에서 한쪽 면은 +1, 다른 면은 -1에 대응된다.
- 가령, weight의 feature가 free와 money라고하면 1차원 방정식이 생성되는데, 이 위를 +, 아래를 -라고한다.

<br><br>

### Binary Perceptron학습

<img src="https://user-images.githubusercontent.com/76278794/227793363-75d14a7f-115c-47c1-84a4-e07cb9911cf9.png">

1. 가중치 weight은 0부터 시작한다.
2. 각 학습데이터마다 아래의 시퀀스를 반복한다.

   - 현재 weight으로 학습데이터를 분류한다.
   - <img src="https://user-images.githubusercontent.com/76278794/227793363-75d14a7f-115c-47c1-84a4-e07cb9911cf9.png">
   - 만약 2-1의 분류결과가 맞다면(`w * f(x) >= 0`), weight의 변화는 없다.
   - 만약 2-1의 분류결과가 틀리다면(`w * f(x) < 0`), weight벡터를 조정한다. - 사전에 지정된 feature vector를 더하거나 빼서 `결정 경계로 이동하도록` weight vector를 조정한다.
   - <img src="https://user-images.githubusercontent.com/76278794/227793805-827671f7-c158-447c-a1b0-f2a79721ff25.png">
   - 여기서 만약 `y*`(새로운 데이터의 y값)이 음수라면 `기존 f와 dot product(내적)`한 값을 w에 반영한다.

<br>

<img src="https://user-images.githubusercontent.com/76278794/227793971-67c19ef5-fd77-4beb-a730-5904ba8f6c5f.png">
	- 위 그림에서, w의 결정경계는 초록색 면이다.

<img src="https://user-images.githubusercontent.com/76278794/227793920-aaeab955-316b-4b38-9b80-ba735b948d68.png">
	- `y* f`를 w에 반영하면 보라색 선으로 바뀌고, 결정경계도 가중치 벡터의 수직으로 바뀌게된다.

<br><br>

## Multiclass Decision Rule

- weight이 binary가 아닌 3개 이상의 값을 가질 수 있을 때,
- 각 class별 weight vector를 𝓌𝓎라고 하자.
  <img width="43" alt="스크린샷 2023-03-27 11 32 05" src="https://user-images.githubusercontent.com/76278794/227826557-47492df1-7c0e-487e-92bf-f3c90d372863.png">

<br>

- 𝓎 class의 `score(activation)`은 `𝓌𝓎 ﹒𝑓(𝔁)`이다.
  <img width="92" alt="스크린샷 2023-03-27 11 31 44" src="https://user-images.githubusercontent.com/76278794/227826514-f8aeb104-5c78-4d7b-b8f9-789c38717a80.png">

<br>

- `이 activation이 가장 큰 값이 나오는 지점에서의 𝓎가 해당 가중치 𝓌𝓎의 정답 벡터`가 된다.

  - `내적이 큰 값을 가질수록, 벡터 사이의 각도가 작다는 의미`이므로, class y가 해당 데이터의 분류가 된다.
    <img width="206" alt="스크린샷 2023-03-27 11 31 09" src="https://user-images.githubusercontent.com/76278794/227826457-08b46c7f-b591-4452-93f1-9d44d8d0b2d1.png">

- 𝓌가 3개라고 가정했을때, 이 3개의 가중치는 평면을 아래와 같이 3개의 결정 경계면, `decision boundary`로 나뉘게된다.

<img width="264" alt="스크린샷 2023-03-27 11 32 21" src="https://user-images.githubusercontent.com/76278794/227826594-2c296a2b-560e-4f60-a577-60fe28239996.png">

- `노란 영역에 있는 어느 벡터도 𝓌 ₁에 내적했을때 가장 큰 값을 가지게 될 것이다.`
  - 즉, 𝓌 ₁과 같은 방향을 가리키고 있다는 것을 의미한다.

<br><br>

### Multiclass Perceptron 학습

- 모든 가중치 weight은 0으로 시작한다.
- 훈련 데이터를 1개씩 선택한다.
- 현재 weight을 통해 가장 큰 값을 가지는 y를 찾는다.

- 만약 y가 정답이면 변화는 없다.

<br>

- 만약 오답이면, 가중치 벡터들의 값을 조정한다.
- 오답의 score는 낮춘다.
  <img width="215" alt="스크린샷 2023-03-27 11 58 00" src="https://user-images.githubusercontent.com/76278794/227829878-06931d7f-087f-4bc4-b1be-c593d7933f30.png">

- 정답의 score(activation)는 높이는 보정을 가한다.
  <img width="286" alt="스크린샷 2023-03-27 11 52 32" src="https://user-images.githubusercontent.com/76278794/227829143-a6dfa77d-3b9b-48b9-8993-81009963d7a6.png">

<br><br>

## Perceptron Algorithm

<img width="190" alt="스크린샷 2023-03-27 12 05 54" src="https://user-images.githubusercontent.com/76278794/227830960-9ee15351-e9e9-42a2-968f-a6b17f068e12.png">

- `훈련 데이터 i`의 현재 output(`o`)와 target output(`t`)의 E(`differences`)를 최소화하는 weight을 찾는다.
- 차이의 제곱을 통해 희석을 방지하고, 미분을 위해 1/2를 곱한다.

<br>

- 경사하강법, `Gradient descent`를 사용해 가중치를 조정해나간다.
- 이 값은 `𝟃𝐸/𝟃𝓌 𝑖𝑗`를 조금씩 더해간다.

<br>

### sequences

   <img width="148" alt="스크린샷 2023-03-27 12 23 23" src="https://user-images.githubusercontent.com/76278794/227833104-ed38000b-e0c6-4c3d-b904-3e64580ca941.png">
1. `𝓌 𝑖𝑗`를 랜덤하게 설정한다.
2. t시점의 가중치와 t+1시점의 가중치가 동일할때까지 반복한다.
	1.  학습 데이터 p 패턴을 선택한다.
	2.  입력의 가중합을 계산해 output을 얻는다.
	3.  t + 1 시점의 가중치는 위의 경사하강법을 적용해 `학습률 η`를 곱한 값을 빼서 조정한다.
	4. weight이 바뀌지 않거나, 최대 반복수에 도달할때까지 반복한다.

<br><br>

## Perceptron의 특성

### Separability

<img width="187" alt="스크린샷 2023-03-27 12 31 06" src="https://user-images.githubusercontent.com/76278794/227834107-a71fd0fe-691b-4e07-bc4e-31d51feac42a.png">
- `분리 가능성`
- 학습 데이터에 대해서 완벽하게 올바른 파라미터(weight)가 존재한다.

### Convergence

<img width="352" alt="스크린샷 2023-03-27 12 31 59" src="https://user-images.githubusercontent.com/76278794/227834202-d1b9db55-5f9c-456f-b2c0-8e52f2af65d2.png">

- `수렴성`
- 학습이 separable하면 결국 perceptron은 수렴한다.
- 위의 경우에도 수렴한다고 할 수 있다.

<br><br>

# Multi-layer Perceptron

## 2층 신경망

<img width="670" alt="스크린샷 2023-03-27 12 33 01" src="https://user-images.githubusercontent.com/76278794/227834334-79f4531a-d74b-49d1-a540-25a7ba3c4dcf.png">

- 가중치 w의 층이 2개로 나뉘어 있어서 2-layer라고 한다.
- 앞쪽 3개의 node에서 각각 가중합해서 output을 계산한 것이 가장 오른쪽 노드의 input이 되어 다시 가중합되어 최종 결과가 연산된다.
- 일반적으로, 각 노드의 실제 output함수는 미분이 안돼서 보정이 불가하기때문에 아래와같은 s자 모양 함수를 사용한다.
  <img width="670" alt="스크린샷 2023-03-27 12 33 01" src="https://user-images.githubusercontent.com/76278794/227834334-79f4531a-d74b-49d1-a540-25a7ba3c4dcf.png">

<br>

### 계산식

- `Feedforward 신경망`이라고도 한다.
  <img width="247" alt="스크린샷 2023-03-27 12 37 32" src="https://user-images.githubusercontent.com/76278794/227834934-6595169e-e825-4658-80d3-46f2bbe73748.png">

- `신경망에 중간층이 최소 1개라도 존재하면 어떤 함수도 표현할 수 있다.`라는 이론적 검증이 되어있다.

- 네트워크, 데이터 모델링, 분류, 예측, 이미지 및 데이터 압축과 패턴 인식에 굉장히 자주 사용된다.

<br><br>

### Back Propagation 알고리즘

- 오늘날 사용되는 딥러닝은 모두 이 알고리즘을 사용한다.

<img width="375" alt="스크린샷 2023-03-27 12 48 56" src="https://user-images.githubusercontent.com/76278794/227836370-8b0c91a2-56a4-40d8-888b-30943000809d.png">

- perceptron처럼 기본적으로 경사하강법(`gradient descent`)를 적용한다.
- 단층 신경망에서는 input과 output의 차이 E를 쉽게 찾을 수 있지만, 다층 신경망에서는 hidden layer의 노드의 E를 찾을 방법이 필요하다.
  - `𝟃𝐸/𝟃𝓌 𝑖𝑗`와 더불어 `𝟃𝐸/𝟃𝓌 𝑗𝑘`도 필요하다는 의미.

<br>

- 그래서, `output node의 E를 중간노드로 뒤로 전파(back propagation)`하는 방법으로 E를 중간노드에 반영하는 방법이 생겼다.

<br>

### Sequences

1. 𝓌 𝑖𝑗, 𝓌 𝑗𝑘를 랜덤 값으로 초기화한다.
2. 모든 hidden layer에 대해 `𝓌 𝑖𝑗, 𝓌 𝑗𝑘가 수렴하거나 원하는 성능이 나올때 까지 반복`
   1. 훈련 데이터로부터 패턴 p를 선택한다.
   2. input으로부터 output을 계산한다.
   3. t+1시점 가중치 조정
      - `𝓌 𝑖𝑗(t+1) = 𝓌 𝑖𝑗(t) - (-η)(𝟃𝐸/𝟃𝓌 𝑖𝑗))`
      - `𝓌 𝑗𝑘(t+1) = 𝓌 𝑗𝑘(t) - (-η)(𝟃𝐸/𝟃𝓌 𝑗𝑘))`

<br><br>

## N층 신경망

- back propagation을 사용한 n층의 신경망을 만들 수 있다.

<img width="678" alt="스크린샷 2023-03-27 12 56 35" src="https://user-images.githubusercontent.com/76278794/227837188-ed33a0a6-319c-4b07-8fbb-8178c315535c.png">
