---
title: "신경망(2) : Deep learning, CNN"
excerpt: "신경망을 기반으로 놀라운 성능을 내는 딥러닝에 대해 이해하기"

categories:
  - ai
tags:
  - deep learning
  - machine learning
  - CNN

toc: true
toc_sticky: true

date: 2023-04-01
last_modified_at: 2023-04-01
---

# Deep Learning

## Perceptron recap

<img width="732" alt="스크린샷 2023-04-01 13 36 55" src="https://user-images.githubusercontent.com/76278794/229265851-80c83995-af59-461f-9504-5cb492969a26.png">

- 퍼셉트론에서 여러 층의 신경망을 N층 신경망이라고하는데, 각 신경세포는 입력값에 대해 가중치를 가해 비선형적인 함수를 거쳐 출력을 낸다.
- 각 출력은 다음 층의 신경세포의 입력으로 사용되며 가중합을통해 비선형함수를 통해 출력이 반복된다.

<br>
<br>

## why deep learning?

- 여러 층을 쌓는다는게 무슨 의미일까?
  - 신경망은 층을 여러개 쌓으면 학습이 잘 안되어 `hidden layer를 1개만 사용하는 2층신경망을 사용했었다.`

<br>

- 그러다보니, 입력으로 사용하는 값을 그대로 사용하지말고 입력을 출력으로 변환하는 그 `비선형함수를 어떻게 잘 설계할 수 있을까?`가 중요했다.
- 어떻게 주어진 문제를 잘 해결하는 특징을 잘 선택할 수 있을까? 가 주요한 문제였다.
- 어떻게 `feature extractor`를 잘 설계할 수 있을까가 문제였다.

<br>

<img width="566" alt="스크린샷 2023-04-01 13 40 51" src="https://user-images.githubusercontent.com/76278794/229265981-b880af03-e7a3-4460-8620-c5451574678f.png">

- 답은 모르지만, 복잡한 함수를 x로부터 찾는게 아닌, `sin, log, cos 등 기본 함수의 조합을 통해 복잡한 함수를 찾는 것`이 효율적이라고 생각했었다.

  - 하지만, 이 방식은 만들고자하는 함수가 기본함수로만 이뤄지지 않을수도 있다는 문제점이 있었다.

<br>

<img width="833" alt="스크린샷 2023-04-01 13 41 20" src="https://user-images.githubusercontent.com/76278794/229265998-0b24e547-c5ab-4f77-b86f-c0d1d95144de.png">
- 그래서 이 함수의 `비선형적인 특징을 학습에서 찾아내보자`가 핵심이 되었다.
- 간단한 함수를 여러개 중첩해서 이 비선형 함수를 찾아낼 수 있을 거라 생각했었다.

<br><br>

## Representation Learning(특징표현학습)

- 딥러닝의 핵심
  - `input에서 output을 표현하기 위한 수많은 선형-비선형 변환의 중첩`

<br>

- 함수를 신경세포에 대입해보면, 신경망과 비슷한 느낌.

<br>

- `층이 진행됨에 따라 디테일한 특징이 표현`된다.
  - `식별하는 특징을 강조`
  - `분류문제에 관계없는 차이는 약화`

<br>
<br>

## Overview

### Deep learning

<img width="699" alt="스크린샷 2023-04-01 14 37 01" src="https://user-images.githubusercontent.com/76278794/229267674-1258e00f-5200-423a-972f-0e39f177bb35.png">

- `뉴런을 학습시키는게 아니라, input->output 함수를 이루는 기본함수들을 학습시키자! 가 기본`
- 이 딥러닝 모델에는 크게 3가지 모델이 있다.

### CNN

<img width="325" alt="스크린샷 2023-04-01 14 37 43" src="https://user-images.githubusercontent.com/76278794/229267687-450499fd-3ce5-4eed-b0bc-b58b5436cfdc.png">

- Convolution Neural network
- 영상처리에 사용
- SGD, ReLU, Dropout, Inception, ResNet등 변형이 생겨남

### GAN

<img width="224" alt="스크린샷 2023-04-01 14 37 52" src="https://user-images.githubusercontent.com/76278794/229267697-bc2f301b-b11c-42ef-ab6a-cd584e03d049.png">

- Generative Adversrial Network
- 중간 특징을 생성해나가면서 해결한다.
- 중간의 특징이 특별한 의미를 가진다.

### RNN

<img width="317" alt="스크린샷 2023-04-01 14 37 58" src="https://user-images.githubusercontent.com/76278794/229267703-cbb15167-cc5f-46c0-acc8-323f411d90f0.png">

- Recurrent Neural Network
- 현재 input을 이전 input을 고려한 self loop라고 하는 가중치로 표현한다.
- LSTM, GRU, Encoder-decoder, Attention등 변형모델 생겨남

<br><br>

# Convolutional Neural Networks(CNN)

## 특징

<img width="889" alt="스크린샷 2023-04-01 14 47 51" src="https://user-images.githubusercontent.com/76278794/229268075-89f600ee-1d74-4d68-89f7-b5ac55b8f7c5.png">

1. `특별한 방법으로 연결하는 정방향 신경망`
2. `input에 convolution(합성곱)` 적용
3. `비선형출력(non-linear activation)`
4. `특징을 줄이는 pooling(subsampling)`
   - 불필요정보 줄이면서 가중 수 감소

<br>

## 1.Convolution(합성곱)

- 각 층은 이전 층의 일정 부분을 통합한다.
  - 큰 데이터를 압축해 목적에 맞는 특징으로 변환한다.
    -> `multiple filter`를 통해 특징을 추출한다.
  - `local receptive field`를 통해 지역적인 부분의 데이터 처리
    -> 근처 화소와의 관계에 대한 특징을 잘 표현한다.

<br>

<img width="782" alt="스크린샷 2023-04-01 14 56 11" src="https://user-images.githubusercontent.com/76278794/229268376-807da453-cf82-49ab-a84a-93af9812b552.png">

- MLP보다 적은 수의 가중치 수를 가진다.
  -> 위에서 봤듯 local receptive field를 사용하기 때문.
  -> 필터간 가중치를 공유하고있다.

<br>

## 2.Non-linearity

- Non-linear activation

### sigmoid function

<img width="272" alt="스크린샷 2023-04-01 15 01 01" src="https://user-images.githubusercontent.com/76278794/229268551-bf85eb87-33bd-4857-bcd9-60cdc0c35344.png">
<img width="214" alt="스크린샷 2023-04-01 15 01 29" src="https://user-images.githubusercontent.com/76278794/229268565-5598f649-523b-485f-b088-fb787c6ff3b6.png">

### Tanh(hyperbolic tangent)

<img width="222" alt="스크린샷 2023-04-01 15 01 53" src="https://user-images.githubusercontent.com/76278794/229268574-5c9c3131-d8b5-470d-b0fd-994455f6fcd5.png">

### !!ReLU(Rectified Linear)!!

- `max(0,x)`
  <img width="266" alt="스크린샷 2023-04-01 15 02 07" src="https://user-images.githubusercontent.com/76278794/229268584-9f3897b3-61db-4d88-a5ae-2b14e4495249.png">

- `backpropagation 과정을 간단하게 만들어 처리 속도를 높임.`
- `feature를 분포시켜 더 잘 분류되게함`
- `activation을 다음 층에 전달해 이전 층의 특징이 점점 희석되는 gradient vanishing문제 해결`

<br>

## 3.Pooling

- 최근에 피함
- 데이터를 압축하고 유연하게한다.
- 디테일한 정보를 삭제.
- 이 삭제 방법은 receptive field의 값을 선택하는 방법에 따라 나뉜다.

<br>

- 사진을 pooling한다고 생각하자.
- 224짜리를 112로 바꿔야한다.

<img width="560" alt="스크린샷 2023-04-01 15 09 39" src="https://user-images.githubusercontent.com/76278794/229268892-466e3f40-5204-4410-9197-19d2990711db.png">

<br>
- 224짜리를 112로 바꿔야한다.

<img width="259" alt="스크린샷 2023-04-01 15 10 28" src="https://user-images.githubusercontent.com/76278794/229268927-1ccafb1d-0233-4de1-9c99-5f8dbdd1877e.png">

### 1. Max-pooling

- receptive field에서 max값으로 pooling을 하면 `Max-pooling`,

### 2. Average-pooling

- average값으로 pooling하면 `Average-pooling`이라고 한다.

## Summary

- CNN

  1.  학습데이터로부터 자동으로 특징을 추출하고 학습한다.
  2.  데이터로부터 분류함수를 매핑한다.

- ConvNet = `몇개의 Convolution(합성곱), Pooling(풀링) 레이어 + 완전 연결 레이어`
- Activation Function(non-linear) = `tanh, sigmoid, ReLU`
- ReLU가 가장 많이 사용됨

<br>

<img width="830" alt="스크린샷 2023-04-01 15 15 22" src="https://user-images.githubusercontent.com/76278794/229269181-25a273d5-457b-4e12-8cdf-b87f38367ff7.png">

<br><br>

# CNN guidelines

## 1. using ReLU

<img width="283" alt="스크린샷 2023-04-01 15 36 47" src="https://user-images.githubusercontent.com/76278794/229269986-4fc9591e-a1c3-4696-be6a-daeb5acad624.png">

- CNN에서 사용되는 비선형 activation 함수
- 연산 속도가 빠르다.
- 생물학적으로도 이렇게 작동한다는 연구
- sigmoid함수가 s자 모양이라 무한한 z에 대해서도 비슷한 a값을 가지는 문제가 있는데, 이를 해결.
- 미분시의 gradient vanishing 문제를 해결한다.

<br>

- Vanishing Gradient Problem

<img width="332" alt="스크린샷 2023-04-01 15 39 05" src="https://user-images.githubusercontent.com/76278794/229270065-be20fe09-aa09-4318-b61d-f3653d4b57ac.png">

- sigmoid 함수 사용하면 발생하는 문제
- input value가 커질수록 output의 기울기가 작아지는 문제.
- 학습이 매우 느리다.
- 랜덤하다.

<br><br>

## 2. Stochastic Gradient Descent

- `Batch Gradient Descent(BGD)`
  - 모든 학습 데이터를 사용해 가중치를 수정해나간다.
  - 많은 연산을 요구로한다.

<br>

- `Stochastic Gradient Descent(SGD)`

  - `학습 데이터의 일부분(mini batch)을 가지고 가중치를 수정해나간다.`
  - 연산이 적다.
  - 반복이 진행됨에따라 BGD와 거의 비슷한 결과를 가진다.
  - BGD보다 나은 성능을 기대할 수 있다.

- 성능지표

<img width="510" alt="스크린샷 2023-04-01 15 43 57" src="https://user-images.githubusercontent.com/76278794/229270302-f599f7de-724d-427a-b33f-9d31a117f838.png">

<br><br>

## 3. Dropout

<img width="581" alt="스크린샷 2023-04-01 15 48 54" src="https://user-images.githubusercontent.com/76278794/229270498-d904a1fc-9e98-4ac0-bfc5-e6bbcb327f3c.png">

- 각 학습마다 connection을 임의로 삭제한다.
- drop의 비율을 `hyper-parameter`로 사용자가 임의로 정한 후 사용한다.
  - dropout은 주어진 학습데이터를 기억하지 않기때문에 `과적합(overfitting) 문제를 방지`한다.

<br><br>

## 4. Optimizers

### Gradient descent

<img width="299" alt="스크린샷 2023-04-01 17 39 48" src="https://user-images.githubusercontent.com/76278794/229275635-3ec5a4f7-ef62-4a70-bb0b-6b21d10fda24.png">

- t + 1시점의 기울기는 t 시점의 기울기에서 함수의 결과를 보정하게된다.
- 이때 함수의 결과에 `학습률 η`를 곱해 보정하는데, 고정된 학습률은 학습의 저하를 불러일으킨다.
- 아래는 모두 이 학습률을 변동성있게 가져가는 방법에 대한 것이다.

<br>

### Momentum

<img width="315" alt="스크린샷 2023-04-01 17 47 35" src="https://user-images.githubusercontent.com/76278794/229275958-05f4ea74-5a04-4a09-8554-39525df13239.png">

- 이전 gradient를 계산에 포함한다.
- 국소최소점 빠지는 것을 방지한다.

<br>

### AdaGrad

<img width="431" alt="스크린샷 2023-04-01 17 58 14" src="https://user-images.githubusercontent.com/76278794/229276450-3f406d64-be58-4088-aa7f-7b0365a245ad.png">

- 업데이트 빈도를 반영해 자주 업데이트될수록 학습률을 작게한다.

<br>

### RMSProp

<img width="407" alt="스크린샷 2023-04-01 17 58 55" src="https://user-images.githubusercontent.com/76278794/229276484-9646b2af-202f-4030-a0f9-92cd0225b35b.png">

- AdaGrad 학습률이 점점 작아지는 문제가 있는데, 일부만 사용하는 방법
- 과거 정보를 일정부분만 반영한다.

<br>

### ADAM

<img width="453" alt="스크린샷 2023-04-01 18 00 11" src="https://user-images.githubusercontent.com/76278794/229276546-fc369ec4-7d11-44bc-a95f-a00e3eac4510.png">

- 가장 보편적으로 사용하는 방법이다.
- 가장 최근에 발표되었다.
- RMSProp과 Momentum을 결합한다.
- Gradient와 Gradient제곱을 모두 사용한다.
- Decaying rate도 두 곳에 모두 사용한다.

<br><br>

## 5. Batch Normalization

- 각 층에서 mini batch단위로 평균 분산으로 보정하는 정규화 과정을 한다.
- 학습데이터와 테스트데이터의 분포가 다를 수 있기 때문에 이를 보정하기 위한 것.

<img width="274" alt="스크린샷 2023-04-01 18 46 52" src="https://user-images.githubusercontent.com/76278794/229278854-e36eafac-1e2e-4a8b-928d-59c9d689f59b.png">

- `Internal covariance shift problem`
  - activation layer간 분포의 차이
  - 학습 및 테스트 데이터가 다른 분포를 가질 수 있다.
  - 작은 학습률을 요구로한다.

<br>

- `Batch normalization`
  - 각 레이어의 각 Minibatch에서 input 특징의 분산을 정규화한다.
  - `scale`과 `shift`를 학습한다.
  - 더 높은 학습률이 가능하다.

<img width="592" alt="스크린샷 2023-04-01 18 49 43" src="https://user-images.githubusercontent.com/76278794/229278998-f6767d62-9c6b-44af-a636-d59867bbe11f.png">

<br><br>

## 6. Ensemble(추가적)

<img width="502" alt="스크린샷 2023-04-01 18 53 46" src="https://user-images.githubusercontent.com/76278794/229279209-8daabf92-3c76-4987-ada6-cd6ca199d5c4.png">

- 정의

  - `여러 모델을 합쳐서` 만드는 모델

- 사용

  - `Bagging`: 분산을 줄이기 위해 사용
  - `Boosting`: 편향을 줄이기 위한 방법
  - `Stacking`: 예측률을 개선

- CNN에 Ensemble을 사용하면 분류 성능을 높일 수 있다.ex)AlexNet

<br><br>

## 7. Model structure

- CNN의 성능을 개선

  - 신경망의 depth, width를 높인다.

- 문제

  1.  Overfitting: 파라미터(가중치)의 개수가 빠르게 증가한다.
  2.  Computation bottleneck: 연산을 많이 필요로된다.

- 해결
  1.  `Uniform structure` : 병렬 컴퓨팅 최적화
  2.  `Sparse structure`: 대칭성을 없애고, 학습을 개선한다.

<br><br>

### Depth of CNN Models

<img width="833" alt="스크린샷 2023-04-01 18 58 34" src="https://user-images.githubusercontent.com/76278794/229279452-ca054774-2499-4af9-a901-3626877fff01.png">

1. AlexNet: 8 layer
2. GoogleNet: 22 layer
3. ResNet, 152 layer

<br><br>
