<h1> chat gpt 원리와 활용방안 </h1>

# 배경

## 기술-> 사회-> 사람

- 기술->사회->사람의 순서로 기술이 사회에 영향을, 사회가 사람에 영향을 주게되었다.

- 예시
  - 증기기관 -> 기계화 혁명-> 사람의 노동 감소
  - 전기 -> 대량생산 혁명 -> 풍족한 물건
  - 컴퓨터와 인터넷 -> 정보화 혁명 -> 손쉬운 정보의 생산과 공유
  - 인공지능 -> 지능화 혁명 -> 필요한 일을 스스로 찾아서 하는 기술

<br>

## 가속도가 붙는 기술의 발전

5000만 사용자를 모으는 데에 걸린 시간을 보면

- 비행기: 68년
- 자동차: 62년
- 신용카드: 28년
- TV: 22년
- 아이팟: 4년
- 페이스북: 3년
- 트위터 2년

기술이 점점 빠른 시간안에 사용자를 모으고 있다.

chatGPT는 `2개월 미만`에 모았다.

<br>

## ChatGPT

- 2022년 11월 30일에 공개되어
  - 5일만에 100만
  - 40일만 1000만
  - 2달만에 MAU 1억명을 달성했다.(Tictok은 9개월)

<br>

## 흥행의 이유

- 누구나 무료로 쉽게 사용가능하다.

  - 일반인도 단순 질의로 답변을 받을 수 있다.
  - 알파고, AI 스피커, AI 챗봇 보다 높은 접근성

- 기존 AI서비스 대비 강력한 성능
  - 대화형 질의에 대한 자연스러운 답변
  - 광범위한 부분에서 강력한 성능
    - 논문
    - 프로그래밍
    - 언여 번역, 교정
    - 문장요약
    - 문장 감정 분석
    - 다양한 컨텐츠를 창의적으로 제작

<br>
<br>

# 원리

## 정의

- Chat + GPT
  - Chat: 챗봇 서비스
  - GPT(Generative Pre-trained Transformer): 사전 학습된 생성 변환기

<br>

- GPT:

  - OpenAI가 만든 대형 언어 모델(LLM, Large Language Model)
  - 굉장히 큰 언어모델이다.

- OpenAI가 만들긴했지만 대부분의 지원을 MS에서 했다.

<img width="516" alt="스크린샷 2023-03-22 18 41 34" src="https://user-images.githubusercontent.com/76278794/226863166-f483b24d-dbb4-46f2-8659-be06a321ba91.png">

- 기본적으로 Transformer가 있는데, 입력을 넣어서 원하는 출력을 만들기 위해서 신경망이라는 파라미터를 넣어서 만든다.
- 여러가지 기능을 넣어서 만들었고, Nx의 의미는 이러한 계층을 N번 쌓는데, 12번 이상까지도 쌓는다.

- Transformer를 왼쪽에 encoder에 넣는데, 파라미터를 넣었을때, Input 값에 랜덤하게 `MASK`로 빈칸을 넣어서 올바른 문장이 출력되도록 하는 신경망이 구성되어있다.

<br>

## LM, 언어 모델이란?

- 정의

  - 문장이 얼마나 자연스러운지 확률적으로 계산해서
  - 문장 내 특정 위치에 나오기 적합한 단어를 예측하는 모델

- 예시

  - Can you please come `here`?
  - 4개의 Can, you, please, come을 보고 `here`라는 단어가 가장 적합하다고 예측한다.

- 확률적으로 하기때문에 과거 데이터가 중요한 역할을 하게된다.

- 학습
  - 특정 단어 뒤에 나올 단어를 예측하게끔한다.
  - Can you -> AI -> 출력-1 (please)
  - Can you please -> AI -> 출력-2(come)
  - Can you please come -> AI -> 출력-3(here)
  - 정확도 높은 데이터가 많을수록 AI의 예측 정확도가 상승한다.
  - 확장: 이전단어 -> 다음단어, 이전 구절 -> 다음구절, 질문 -> 답변, 빈칸채우, 키워드 자동완성, 챗봇 등
    - 이렇게 많은 데이터를 넣으면 단어 단위가 아닌, 큰 단위로 모델을 만들 수 있다.

<br>

## LLM 대규모 언어 모델

- 전통적인 확률 테이블로 만들면 문제가 있다.
- 확률의 문제이기도 하고, 방대한 데이터 내에서 미묘한 변화를 캐치하기가 어려웠다.
- 그래서 대규모로 하게하기위해서 신경망 기반 언어모델이 생겼다.

### 신경망 기반 언어모델

<img src="https://user-images.githubusercontent.com/76278794/227089456-2a57a0a7-448c-4222-ac5b-22c8a63ed1ea.png">

- input으로 단어를 넣고, output layer에서 원하는 단어가 나올때 확률값, 중앙의 단어를 앞 뒤 단어로 추정하는 등 가능성이 있다.
- transformer로 학습을 하면서 masked learning이 생겼다.

<br>

### 자가학습(masked learning)

<img src="https://user-images.githubusercontent.com/76278794/227090034-d93e64e5-50aa-4486-a891-88ce7d974392.png">

- 인공지능에서 학습하는 방법이 지도, 비지도, 강화학습이 있는데, 자가학습은 최근에 생긴것.
- 단어들 사이 연관관계를 모델화한다. 입력 중간중간을 masked해서 비워두고 올바른 출력을 유도한다.

<br><br>

## ChatGPT의 학습 원리

- ChatGPT는 2 단계로 학습이 이루어졌다.
  - 학습: 많은 양의 데이터 입력으로 올바른 출력을 결정하는 파라미터를 결정

<br>

### 1. 사전학습

<img src="https://user-images.githubusercontent.com/76278794/227092020-ec5c0112-a758-4584-ac0b-339b3ff9bfcb.png">

- Large corpus를 통한 학습
- `비지도학습`에 의한 언어모델링
  - 특정 문장에 대한 답을 유추하는 것이 아니라, 원본 문장을 스스로 마스킹하기때문
- 원본 텍스트를 마스킹하고, 예측된 텍스트를 기반으로 원본과 비교해 Loss가 최소가되게끔 학습
- 2021년 이전 데이터만 있다.
  - 다양한(웹, 책, 논문)
  - 대용량(45TB)

<br>

### 2. 미세조정

<img src="https://user-images.githubusercontent.com/76278794/227092249-618959ce-b0da-424e-be88-6c578ddbeca6.png">

- 라벨링된 데이터를 통해 정답을 유추한다.
- 프롬프트 샘플링과 40명이 작성한 정답기반 `지도학습`
- 그리고 학습된 `보상 모델`의 응답기반 `강화학습`
  - 4 ~9 개 중 응답 중에서 사람이 응답의 선호도를 보상함수로서 작동하게한다.

<br>

## ChatGPT 학습 변천사

1. `Open AI GPT-3`

   - 2020년 5월

2. `Instruct GPT`

   - 2020년 5월
   - 인간의 피드백을 통한 강화학습 적용
   - 기능 향상 및 사용자 의도파악

3. `Chat GPT`
   - Instruct GPT가 사람에 유해한 것도 학습하기때문에 이를 개선
   - 무해성을 개선하며 유해성과 무해성간 trade-off를 관리한다.

<br>

### InstructGPT

- `인간의 피드백`을 통한 강화학습 적용 목표

1. 설명 데이터(`demonstration data`) 수집하고, `사람이 policy를 정해서 원하는 동작`을 구현한다.
2. `comparison data`를 수집하고, 보상모델을 학습해 사람이 `선호하는 출력을 예측`하는 모델을 만든다.
3. PPO알고리즘(지도 정책의 미세조정)기반으로 `보상 모델 원칙을 최적화`한다.

<br>

### ChatGPT

- 유해성 완화가 목표

1. 크라우드 워커(사람)를 통한 유해 응답 탐색 및 해당 응답 정정

   - 유해한 질문에 대해서 유해한 응답이 나오면 사람이 직접 1. 스스로 비평하고 2. 수정요청
     을 통해 응답을 정정한다.

2. 유용성에 대한 사람과 인공지능 모델에 대한 trade-off관리
   - 질문에 대한 응답을 받을때, `CHAIN-OF-THOUGHT`를 사용했다.
   - A라는 질문이 있다면, 응답을 세세하게 나누어
     - A-B(a)
     - A-B(a)-B(b)
     - A-B(a)-B(b)-B(c)
     - 이렇게 중간 답변을 input으로 넣어가면서 맥락을 관리하게했다.

<br>

## 품질관리

- 결국 품질관리를 위해 사람의 역할이 절대적으로 중요하다.
- 리뷰어의 피드백에 따른 미세조정된 데이터와 사전학습된 모델간 trade-off를 사람이 결정해서 만들어졌다.

<img src="https://user-images.githubusercontent.com/76278794/227096872-d94beb16-fa5d-49b0-bc84-5139abe6a04c.png">

<br>
<br>

# 응용사례

- 지식 검색 및 요약
- 스토리 생성
- 자동 코딩

<br>

# 비용

1. 미세조정 위한 데이터생성 비용
   - 사람에 의한 질문 혹은 응답 생성
   - 선호도 평가비용
2. 대규모 병렬학습
   - 클라우드 컴퓨팅 기반
   - 1만개의 Nvidia GPU로 수주간 학습이 필요하다.
   - 1회 학습: 지구<->달 왕복 차량 주행 만큼의 이산화탄소 배
3. 응답생성 비용 - 1억명 2개월만에 달성 - 30단어당 생성 비용 약 1.5원

<br>

# 한계

- 기능적

  - hallucination으로 답변 정보가 모두 정확한것이 아님.
  - 2021년이전데이터기때문에 최신내용은 불가능
  - 답을 찾는 것이 아닌, 답을 생성하는 형태로 오답생성

- 윤리적

  - 사람의 개입은 강점이자 취약점
  - 인간 피드백으로 인해 유해, 편향답변최소화하나, 불완전

- 비용적

  - 운용 관리에 많은 비용이 지속적 소요됨

- 환경적
  - 학습, 운용에 온실가스 배출

<br>

# 전망

## 노동의 미래

- AI를 잘하는 사람은 기회, 못하는 사람은 위기
  - AI가 보조
    - 접객, 영업, 교육, 기획, 집필 업무
  - AI가 확장
    - 고도의 전문, 예측분석 업무
  - AI를 보조
    - 데이터 입력, 전화응답, 운전, 운반
  - AI로 사라지는 업무
    - 주문, 회계, 감시업무

<br>

## 공공의 미래

- 행정효율성
- 국민 편의 증진
- 업무 활용시 주의사항
  - 정량 답이 필요한 업무에는 부적절
  - 최종 결과물 확인이 필요
  - 업무에 맞는 서비스 취사선택해야함

## 언어처리 인공지능 발전

- Transformer(2017)

  - 어센션모델을 사용해 병렬 학습에 유리

- GPT-1(2018)

  - 질문에 어울리는 답변생성, 파라미터 1억개

- GPT-2(2019)

  - 15억개 파라미터로 다양한 스타일, 어조의 답변

- GPT-3(2020.5)

  - 1750억개 파라미터, 다양한 자연어처리

- Codex(2021)

  - 60억 파라미터, 프로그래밍언어중심

- InstructGPT(2022.1)

  - 보상모델을 통한 강화학습 적용, 사람의 피드백 존중

- GPT-3.5(22.11.28)

  - 향상된 모델의 별칭

- ChatGPT(22.11.30)

  - 대화에 특화된 GPT 모델

- GPT-4(23.3.14)

<br>

# GPT4의특징

1. Creativity
   - 창의적, 기술적 글쓰기 작업가능
2. Visual Input(multi-modal 프로세싱)
   - 이미지 입력받아 캡션, 분류가능
3. Longer Context

   - 입력 컨텍스트가 길어짐.
   - 고급 추론 능력

4. 시험통과
   - 변호사시험
   - 생물 올림피아드
   - SAT

<br>

# 결론

- 거대언어모델
  - 소형화, 전문화
  - 외부 DB를 통한 최신자료접근
  - 신뢰성 확보

<br>

- 데이터베이스, 클라우드, 통신, 반도체 등 관련 기술 개발 필요

- 질문을 잘해야함

<br>

# 질문요령

1. 초안작성에 유리
2. 구체적인 내용을 prompt에 줄 것
3. 예시를 보여줘야함
4. 항상 내용을 보고 자세히 읽고 체크
5. 영어로 입력한 것을 출력받아 번역하는게 더 좋은 내용
6. 인터랙티브하게 활용
7. 차별화된 내용을 추가한다.

<br>

- 단, 정확한 값은 기대하면 안된다.
- 정확한 내용이 필요한 글은 내용이 정확해야하는 부분을 반드시 더블체크한다.
- 언어 사용 문화, 법적인 부분은 최신데이터가 없기에 학습이 부족하다.
- 최신지식은 물어보면 안된다.
